{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Previsao de Embarque \n",
        "- Simples \n",
        "- Sem Serie Temporal\n",
        "- Regressao Linear\n",
        "- Random Forest\n",
        "- Com Grid Search\n",
        "- Bagging, Stacking e Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1632360924338
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, StackingRegressor, GradientBoostingRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import make_regression\n",
        "import datetime as dt\n",
        "\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from IPython.display import display\n",
        "from pprint import pprint\n",
        "from math import sqrt\n",
        "\n",
        "from variables import day_of_week_translator, feriados, vesperas\n",
        "\n",
        "#from shutil import unpack_archive\n",
        "#unpack_archive('./df_input.zip', './')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Filtro de Onibus e definicoes de features/target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360927530
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Reading Data file (geolocalized)\n",
        "data = pd.read_csv('../data_input_zerofill_2018.csv')\n",
        "\n",
        "busline_filter = 41\n",
        "data_model = data.copy()\n",
        "\n",
        "#d_ano,  estao disponiveis para inserir once eu tive mais de 1 ano de dados\n",
        "feature_names = [\n",
        "    'hour_sin', 'hour_cos', \n",
        "    'd_mes', 'd_ano', 'mes', 'semana_do_mes', \n",
        "    'domingo','segunda', 'terca', 'quarta', 'quinta', 'sexta', 'sabado', \n",
        "    'feriado', 'vespera_feriado']\n",
        "\n",
        "target = 'validations_per_hour'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1632360927873
        }
      },
      "outputs": [],
      "source": [
        "data_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1632360940056
        }
      },
      "outputs": [],
      "source": [
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "data_model[['domingo','segunda', 'terca', 'quarta', 'quinta', 'sexta', 'sabado']] = one_hot_encoder.fit_transform(data_model['d_semana'].values.reshape(-1,1))\n",
        "data_model['feriado'] = [1 if any((x[0] == d and x[1] == m) for (_, d, m) in feriados) else 0 for x in list(zip(data_model.d_mes, data_model.mes))]\n",
        "data_model['vespera_feriado'] = [1 if any((x[0] == d and x[1] == m) for (_, d, m) in vesperas) else 0 for x in list(zip(data_model.d_mes, data_model.mes))]\n",
        "\n",
        "top100_linhas = data_model.linha.value_counts().index[:100]\n",
        "top100_linhas_data_model = data_model[data_model.linha.isin(top100_linhas)].loc[data['mes'] != 8]\n",
        "\n",
        "line_data_model = data_model.loc[data['linha'] == busline_filter].loc[data['mes'] != 8].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top100_linhas_data_model.to_csv(\"../dados-para-modelos/2018/top100_linhas_data_model_2018.csv\", index=False)\n",
        "sns.set(rc={'figure.figsize':(15,10)})\n",
        "\n",
        "sns.heatmap(top100_linhas_data_model.corr()[['validations_per_hour']].sort_values(by='validations_per_hour', ascending=False)\n",
        ", vmin=-1, vmax=1, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360940459
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#display(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360940845
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#display(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "---------------------------------------------------------------------------- Fim Setup ----------------------------------------------------------------------------  \n",
        "# Criacao dos Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360941131
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360941444
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_performance(model):\n",
        "    y_test_predict = model.predict(X_test)\n",
        "    mse = mean_squared_error(Y_test, y_test_predict)\n",
        "    rmse = (np.sqrt(mse))\n",
        "    r2 = r2_score(Y_test, y_test_predict)\n",
        "    mae = mean_absolute_error(Y_test, y_test_predict)\n",
        "    mape = mean_absolute_percentage_error(Y_test, y_test_predict) * 100\n",
        "    performance_scoring = [\n",
        "        r2,\n",
        "        rmse,\n",
        "        mae,\n",
        "        mape\n",
        "    ]\n",
        "    #performance_scoring = pd.DataFrame(performance_scoring,columns=['Metrica', 'Score'])\n",
        "    #performance_scoring['Score'] = performance_scoring['Score'].astype('float64')\n",
        "    return performance_scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360941799
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "LinearRegressionModel = LinearRegression()\n",
        "RandomForestModel = RandomForestRegressor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Linear Regression Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360942004
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# print(\"Linear Regression  Params ----> \")\n",
        "# pprint(LinearRegressionModel.get_params())\n",
        "\n",
        "copy_X = [True, False]\n",
        "fit_intercept = [True, False]\n",
        "n_jobs = [2, 4, 6, 8]\n",
        "normalize = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid_linearregression = {'copy_X': copy_X, 'fit_intercept': fit_intercept, 'n_jobs': n_jobs, 'normalize': normalize}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Random Forest Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632360942214
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# print(\"Random Forest Model Params ----> \")\n",
        "# pprint(RandomForestModel.get_params())\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 1000, num = 10)]\n",
        "\n",
        "n_jobs = [2, 4, 6, 8]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid_randomforest = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap,\n",
        "               'n_jobs': n_jobs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632361324253
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "X = line_data_model.filter(feature_names, axis=1)\n",
        "y = line_data_model.validations_per_hour\n",
        "\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "LinearRegressionModel = LinearRegression()\n",
        "RandomForestModel = RandomForestRegressor()\n",
        "gridsearch_result_randomforest = RandomizedSearchCV(estimator = RandomForestModel, param_distributions = random_grid_randomforest, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "gridsearch_result_linearregression = RandomizedSearchCV(estimator = LinearRegressionModel, param_distributions = random_grid_linearregression, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "\n",
        "# Fit the random search model\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "\n",
        "gridsearch_result_randomforest.fit(X_train, Y_train)\n",
        "gridsearch_result_linearregression.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Random Forest Best Parameters -----> \")\n",
        "pprint(gridsearch_result_randomforest.best_params_)\n",
        "\n",
        "print(\"Linear Regression Best Parameters -----> \")\n",
        "pprint(gridsearch_result_linearregression.best_params_)\n",
        "\n",
        "gridsearched_random_randomforest = gridsearch_result_randomforest.best_estimator_\n",
        "gridsearched_random_linearregression = gridsearch_result_linearregression.best_estimator_\n",
        "\n",
        "default_linearregression_model = LinearRegressionModel.fit(X_train, Y_train)\n",
        "default_randomforest_model = RandomForestModel.fit(X_train, Y_train)\n",
        "\n",
        "GridSearchedRandomForestModelPipeline = make_pipeline(gridsearched_random_randomforest)\n",
        "DefaultRandomForestModelPipeline = make_pipeline(default_randomforest_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "performance_default_randomforest_model = get_performance(default_randomforest_model)\n",
        "performance_gridsearched_randomforest_model = get_performance(gridsearched_random_randomforest)\n",
        "performance_gridsearched_linearregression_model = get_performance(gridsearched_random_linearregression)\n",
        "performance_default_linearregression_model = get_performance(default_linearregression_model)\n",
        "\n",
        "print(\"\\n----------------------------------------------------\\n\")\n",
        "print('Regressao Linear [DEFAULT]: \\n', performance_default_linearregression_model)\n",
        "print(\"\\n----------------------------------------------------\\n\")\n",
        "print('Random Forest [DEFAULT]: \\n', performance_default_randomforest_model)\n",
        "print(\"\\n----------------------------------------------------\\n\")\n",
        "print('Regressao Linear [GRID SEARCH]: \\n', performance_gridsearched_linearregression_model)\n",
        "print(\"\\n----------------------------------------------------\\n\")\n",
        "print('Random Forest [GRID SEARCH]: \\n', performance_gridsearched_randomforest_model)\n",
        "print(\"\\n----------------------------------------------------\\n\")\n",
        "\n",
        "performance_total = [performance_default_linearregression_model,\n",
        "    performance_default_randomforest_model,\n",
        "    performance_gridsearched_linearregression_model,\n",
        "    performance_gridsearched_randomforest_model]\n",
        "\n",
        "performance_total = pd.DataFrame.from_records(performance_total, columns=['R2', 'RMSE', 'MAE', 'MAPE'], \n",
        "    index=['LinearReg Default', 'RandomForest Default', 'LinearReg GridSearched', 'RandomForest GridSearched'])\n",
        "performance_total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## BAGGING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632361332525
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "warnings.filterwarnings(\"ignore\") # To ignore warnings\n",
        "n_jobs = -1 # This parameter conrols the parallel processing. -1 means using all processors.\n",
        "random_state = 42 # This parameter controls the randomness of the data. Using some int value to get same results everytime this code is run\n",
        "models_scores = [] # To store model scores               \n",
        "\n",
        "def bagging_model(estimator):\n",
        "    \"\"\"\n",
        "    I/P\n",
        "    estimator: The base estimator from which the ensemble is grown.\n",
        "    O/P\n",
        "    br_y_pred: Predictions on test data for the base estimator.\n",
        "    \n",
        "    \"\"\"\n",
        "    regr = BaggingRegressor(base_estimator=estimator,\n",
        "                            n_estimators=10,\n",
        "                            max_samples=1.0,\n",
        "                            bootstrap=True, # Samples are drawn with replacement\n",
        "                            n_jobs= n_jobs,\n",
        "                            random_state=random_state).fit(X_train, Y_train)\n",
        "\n",
        "    br_y_pred = regr.predict(X_test)\n",
        "\n",
        "    performance = get_performance(regr)\n",
        "    \n",
        "    print(f'Performance for base estimator {regr.base_estimator_} = {performance}\\n')\n",
        "\n",
        "    return regr, performance\n",
        "\n",
        "LinearRegressionModelPipeline = make_pipeline(LinearRegression())\n",
        "RandomForestModelPipeline = make_pipeline(RandomForestRegressor())\n",
        "\n",
        "\n",
        "bagging_lr_model, performance_bagging_lr_model = bagging_model(LinearRegressionModelPipeline)\n",
        "bagging_rf_model, performance_bagging_rf_model = bagging_model(RandomForestModelPipeline)\n",
        "\n",
        "df2 = pd.DataFrame([performance_bagging_lr_model, performance_bagging_rf_model], columns=['R2','RMSE','MAE', 'MAPE'], \n",
        "    index=['LinearRegression Bagging', 'RandomForest Bagging'])\n",
        "performance_total = pd.concat([df2, performance_total])\n",
        "\n",
        "BaggingLinearRegressionModelPipeline = make_pipeline(bagging_lr_model)\n",
        "BaggingRandomForestModelPipeline = make_pipeline(bagging_rf_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# BOOSTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632361361458
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "gradient_boosting_regressor= GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
        "                                   max_depth=4, max_features='sqrt',\n",
        "                                   min_samples_leaf=15, min_samples_split=10, \n",
        "                                   loss='huber', random_state = random_state)\n",
        "\n",
        "gradient_boosting_regressor.fit(X_train, Y_train)\n",
        "\n",
        "performance = get_performance(gradient_boosting_regressor)\n",
        "\n",
        "df2 = pd.DataFrame([performance], columns=['R2','RMSE','MAE', 'MAPE'], \n",
        "    index=['GradientBoostingRegressor'])\n",
        "\n",
        "GradientBoostingModelPipeline = make_pipeline(gradient_boosting_regressor)\n",
        "\n",
        "performance_total = pd.concat([df2, performance_total])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STACKING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estimators = [\n",
        "    ('gridseached_randomforest', GridSearchedRandomForestModelPipeline),\n",
        "    ('bagging_randomforest',BaggingRandomForestModelPipeline),\n",
        "    ('gradient_boosting', GradientBoostingModelPipeline), \n",
        "    ('random_forest_default', DefaultRandomForestModelPipeline)\n",
        "]\n",
        "\n",
        "stack = StackingRegressor(estimators=estimators, final_estimator=DefaultRandomForestModelPipeline, cv= 5, n_jobs= n_jobs, passthrough = True)\n",
        "\n",
        "stack.fit(X_train, Y_train)\n",
        "\n",
        "performance = get_performance(stack)\n",
        "\n",
        "df2 = pd.DataFrame([performance], columns=['R2','RMSE','MAE', 'MAPE'], \n",
        "    index=['Stacking Regressors'])\n",
        "\n",
        "performance_total = pd.concat([df2, performance_total])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_list = [\n",
        "#     stack,\n",
        "#     gradient_boosting_regressor,\n",
        "#     bagging_rf_model,\n",
        "#     default_randomforest_model,\n",
        "#     #default_linearregression_model,\n",
        "#     #gridsearched_result_linearregression,\n",
        "#     #gridsearched_random_randomforest,\n",
        "#     #bagging_lr_model,\n",
        "# ]\n",
        "\n",
        "sample = X.sample(n=1)\n",
        "sample\n",
        "\n",
        "print('LINHA UTILIZADA PARA OS TESTES: ', busline_filter)\n",
        "\n",
        "print(\"\\nUtilizando sample aleatorio de dado para teste de previsao: \")\n",
        "\n",
        "display(data_model.loc[sample.index[0]:sample.index[0]])\n",
        "\n",
        "res = stack.predict(sample)\n",
        "print(\"Stacked Regressor -> resultado do predict de test: \", res)\n",
        "\n",
        "res = gradient_boosting_regressor.predict(sample)\n",
        "print(\"gradient_boosting_regressor -> resultado do predict de test: \", res)\n",
        "\n",
        "res = bagging_rf_model.predict(sample)\n",
        "print(\"bagging_rf_model -> resultado do predict de test: \", res)\n",
        "\n",
        "res = gridsearched_random_randomforest.predict(sample)\n",
        "print(\"Random Forest Best Grid Searched -> resultado do predict de test: \", res)\n",
        "\n",
        "res = default_randomforest_model.predict(sample)\n",
        "print(\"Random Forest Default -> resultado do predict de test: \", res)\n",
        "\n",
        "res = bagging_lr_model.predict(sample)\n",
        "print(\"bagging_lr_model -> resultado do predict de test: \", res)\n",
        "\n",
        "res = default_linearregression_model.predict(sample)\n",
        "print(\"Regressao Linear Default -> resultado do predict de test: \", res)\n",
        "\n",
        "res = gridsearched_random_linearregression.predict(sample)\n",
        "print(\"Regressao Linear Best Grid Searched -> resultado do predict de test: \", res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632369757434
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#performance_total.to_csv('../performances/2018/performance_total_linha'+str(busline_filter)+'.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "performance_total.sort_values('MAE', ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Todos os modelos treinados dentro de *model_list*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1632363793252
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_list = [\n",
        "    stack,\n",
        "    gradient_boosting_regressor,\n",
        "    bagging_rf_model,\n",
        "    default_randomforest_model,\n",
        "    gridsearch_result_randomforest,\n",
        "    #default_linearregression_model,\n",
        "    #gridsearch_result_linearregression,\n",
        "    #bagging_lr_model,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Treinar com 1 mes, prever N semanas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1632364536684
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Jan, Fev, Mar, Abril, Maio\n",
        "# treina com mar, preve 2 semanas de abril\n",
        "\n",
        "mes_de_treino = 4\n",
        "mes_de_previsao = 5\n",
        "\n",
        "line_mes_data_model = line_data_model[line_data_model.mes == mes_de_treino]\n",
        "\n",
        "line_mes_data_model_predict = line_data_model[line_data_model.mes == mes_de_previsao]\n",
        "\n",
        "df_prev = line_mes_data_model_predict[line_mes_data_model_predict.semana_do_mes <  2].filter(feature_names, axis = 1)\n",
        "real_values = pd.DataFrame(line_mes_data_model_predict[line_mes_data_model_predict.semana_do_mes <  2].validations_per_hour)\n",
        "\n",
        "X = line_mes_data_model.filter(feature_names, axis=1)\n",
        "y = line_mes_data_model.validations_per_hour\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "\n",
        "predictions = [[],[],[],[]]\n",
        "performances = list()\n",
        "df_prev\n",
        "for i, model in enumerate(model_list):\n",
        "        model.fit(X_train, Y_train)\n",
        "        performances.append(get_performance(model))\n",
        "\n",
        "        for index, row in df_prev.iterrows():\n",
        "            predictions[i].append(model.predict([row])[0])\n",
        "\n",
        "df_performance = pd.DataFrame(performances)\n",
        "df_performance.to_csv('../performances/2018/linha'+str(busline_filter)+'_1mes('+str(mes_de_treino)+')_1semanas('+str(mes_de_previsao)+')_performance.csv', index=False)\n",
        "df_predict_dados = real_values\n",
        "df_predict_dados['validations_per_hour_predicted'] = predictions[0]\n",
        "df_predict_dados.to_csv('../predict-vs-real/2018/linha'+str(busline_filter)+'_1mes('+str(mes_de_treino)+')_1semanas('+str(mes_de_previsao)+')_predict.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Treinar com (N-Y) meses, Prever mes (N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631467016584
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#X_train, Y_train\n",
        "#X_test, Y_test\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "# X = line_data_model.filter(feature_names, axis=1)\n",
        "# y = line_data_model.validations_per_hour\n",
        "\n",
        "#quantos meses (comecando do final do dado) vou prever\n",
        "\n",
        "# model_list = [\n",
        "#     default_linearregression_model,\n",
        "#     default_randomforest_model,\n",
        "#     gridsearch_result_linearregression,\n",
        "#     gridsearch_result_randomforest,\n",
        "#     bagging_lr_model,\n",
        "#     bagging_rf_model,\n",
        "#     stack,\n",
        "#     gradient_boosting_regressor\n",
        "# ]\n",
        "\n",
        "predictions = [[],[],[],[],[],[],[],[]]\n",
        "performances = [[],[],[],[],[],[],[],[]]\n",
        "\n",
        "df_treinamento = line_data_model[line_data_model.mes < 7].copy()\n",
        "df_previsao = line_data_model[line_data_model.mes == 7].copy()\n",
        "df_prev = df_previsao.filter(feature_names, axis = 1)\n",
        "df_results = df_previsao.filter('validations_per_hour', axis = 1)\n",
        "\n",
        "print(df_results.__len__())\n",
        "\n",
        "X = df_treinamento.filter(feature_names, axis=1)\n",
        "y = df_treinamento.validations_per_hour\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "\n",
        "for i, model in enumerate(model_list):\n",
        "    model.fit(X_train, Y_train)\n",
        "    performances[i].append(get_performance(model))\n",
        "\n",
        "    for index, row in df_prev.iterrows():\n",
        "        predictions[i].append(model.predict([row])[0])\n",
        "\n",
        "df_results = df_previsao.validations_per_hour\n",
        "\n",
        "df_performance = pd.DataFrame(performances)\n",
        "df_performance.to_csv('../performances/2018/performance_6_meses_predict_7th.csv', index=False)\n",
        "\n",
        "plt_size = 6 * 360\n",
        "\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.plot(range(len(df_results) - plt_size), df_results[plt_size:], color='blue')\n",
        "plt.plot(range(len(predictions[0]) - plt_size ), predictions[2][plt_size:], color='red')\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.show()\n",
        "\n",
        "#print(predictions[0])\n",
        "\n",
        "\n",
        "# for hzp in range(4, 8):\n",
        "#     horizonte_de_previsao = hzp\n",
        "#     meses = line_data_model.mes.sort_values().unique()\n",
        "#     horizonte_de_treinamento = meses[:meses.size - horizonte_de_previsao]\n",
        "#     meses_de_previsao = meses[meses.size - horizonte_de_previsao:]\n",
        "\n",
        "#     predictions = [[],[],[],[],[],[],[],[]]\n",
        "#     performances = [[],[],[],[],[],[],[],[]]\n",
        "\n",
        "#     df_treinamento = line_data_model[line_data_model.mes.isin(horizonte_de_treinamento)].copy()\n",
        "#     df_previsao = line_data_model[line_data_model.mes.isin(meses_de_previsao)].copy()\n",
        "#     df_prev = df_previsao.filter(feature_names, axis = 1)\n",
        "#     df_results = df_previsao.filter('validations_per_hour', axis = 1)\n",
        "\n",
        "#     print(df_results.__len__())\n",
        "\n",
        "#     X = df_treinamento.filter(feature_names, axis=1)\n",
        "#     y = df_treinamento.validations_per_hour\n",
        "#     X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "\n",
        "#     for i, model in enumerate(model_list):\n",
        "#         model.fit(X_train, Y_train)\n",
        "#         performances[i].append(get_performance(model))\n",
        "\n",
        "#         for index, row in df_prev.iterrows():\n",
        "#             predictions[i].append(model.predict([row])[0])\n",
        "    \n",
        "#     df_results = df_previsao.validations_per_hour\n",
        "\n",
        "#     df_performance = pd.DataFrame(performances)\n",
        "#     df_performance.to_csv('../performances/2018/performance_'+str(hzp)+'_meses.csv', index=False)\n",
        "\n",
        "#     plt_size = hzp * 360\n",
        "\n",
        "#     plt.figure(figsize=(30, 10))\n",
        "#     plt.plot(range(len(df_results) - plt_size), df_results[plt_size:], color='blue')\n",
        "#     plt.plot(range(len(predictions[0]) - plt_size ), predictions[2][plt_size:], color='red')\n",
        "#     plt.xlabel('True Values')\n",
        "#     plt.ylabel('Predictions')\n",
        "#     plt.show()\n",
        "\n",
        "#print(predictions[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631409753552
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "plt.plot(range(len(df_results) - plt_size), df_results[plt_size:], color='blue')\n",
        "plt.plot(range(len(predictions[2]) - plt_size ), predictions[0][plt_size:], color='red')\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631409753830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Ranking Linhas de Onibus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631409754617
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(data_model.linha.value_counts(ascending=True)[data_model['linha'].value_counts() < 100])\n",
        "\n",
        "# linha_lista = data_model.linha.unique()\n",
        "# linha_lista = linha_lista[:10]\n",
        "res_map = list()\n",
        "for linha in top100_linhas:\n",
        "    currentLinhaData = top100_linhas_data_model.loc[top100_linhas_data_model['linha'] == linha].copy()\n",
        "    X = currentLinhaData.filter(feature_names, axis=1)\n",
        "    y = currentLinhaData.validations_per_hour\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "    for model in model_list:\n",
        "        model.fit(X_train, Y_train)\n",
        "        performance = get_performance(model)\n",
        "        performance.insert(0, linha)\n",
        "        res_map.append(performance)\n",
        "res_map = pd.DataFrame(res_map, columns=[\"Linha\", \"R2\", \"RMSE\", \"MAE\", 'MAPE'])\n",
        "r = res_map.sort_values('MAE', ascending=True)\n",
        "r.to_csv(\"../performances/2018/ranking_top100_2018.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631504114908
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# res_map = pd.DataFrame(res_map,columns=['Linha', \n",
        "#     # '[LR][DEF]R2', '[LR][DF]RMSE', '[LR][DF]MAE',\n",
        "#     '[RF][GS]R2', '[RF][GS]RMSE', '[RF][GS]MAE',\n",
        "#     # '[LR][BAG]R2', '[LR][BAG]RMSE', '[LR][BAG]MAE',\n",
        "#     '[RF][BAG]R2', '[RF][BAG]RMSE', '[RF][BAG]MAE',\n",
        "#     '[STK]R2', '[STK]RMSE', '[STK]MAE',\n",
        "#     # '[BOS]R2', '[BOS]RMSE', '[BOS]MAE' \n",
        "#     ])\n",
        "res_map = pd.DataFrame(res_map,columns=['Linha','R2', 'RMSE', 'MAE', 'MAPE'])\n",
        "\n",
        "#performance_scoring['Score'] = performance_scoring['Score'].astype('float64')\n",
        "res_map = res_map.sort_values('R2').reset_index(drop=True)\n",
        "res_map.to_csv('../top100linhas_rank_2018.csv', index=False)\n",
        "res_map"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "4caa081543ae4d7454eea2adf6270962382d99fc25b3a6423caed7aecce4d7f8"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('ml': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
