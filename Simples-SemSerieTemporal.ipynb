{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Previsao de Embarque \r\n",
        "- Simples \r\n",
        "- Sem Serie Temporal\r\n",
        "- Regressao Linear\r\n",
        "- Random Forest\r\n",
        "- Com Grid Search\r\n",
        "- Bagging, Stacking e Boosting"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, StackingRegressor, GradientBoostingRegressor\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.datasets import make_regression\r\n",
        "import datetime as dt\r\n",
        "\r\n",
        "import warnings\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\n",
        "#from sklearn.metrics import mean_absolute_percentage_error\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from IPython.display import display\r\n",
        "from pprint import pprint\r\n",
        "from math import sqrt\r\n",
        "\r\n",
        "#from shutil import unpack_archive\r\n",
        "#unpack_archive('./df_input.zip', './')"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1632360924338
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtro de Onibus e definicoes de features/target"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Reading Data file (geolocalized)\r\n",
        "data = pd.read_csv('./data_input_zerofill.csv', sep=';', delimiter=';')\r\n",
        "\r\n",
        "busline_filter = 41\r\n",
        "data_model = data.copy()\r\n",
        "\r\n",
        "#d_ano,  estao disponiveis para inserir once eu tive mais de 1 ano de dados\r\n",
        "feature_names = [\r\n",
        "    'hour_sin', 'hour_cos', \r\n",
        "    'd_mes', 'd_ano', 'mes', 'semana_do_mes', \r\n",
        "    'domingo','segunda', 'terca', 'quarta', 'quinta', 'sexta', 'sabado', \r\n",
        "    'feriado', 'vespera_feriado']\r\n",
        "\r\n",
        "target = 'validations_per_hour'"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360927530
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "day_of_week_translator = {\r\n",
        "    0: \"Domingo\",\r\n",
        "    1: \"Segunda\",\r\n",
        "    2: \"Terca\",\r\n",
        "    3: \"Quarta\",\r\n",
        "    4: \"Quinta\",\r\n",
        "    5: \"Sexta\",\r\n",
        "    6: \"Sabado\"\r\n",
        "}\r\n",
        "\r\n",
        "feriados = [\r\n",
        "            ['Ano Novo', 1, 1],\r\n",
        "            ['Carnaval', 24, 2],\r\n",
        "            ['Carnaval', 25, 2],\r\n",
        "            ['Carnaval', 26, 2],\r\n",
        "            ['Dia de São José', 19, 3],\r\n",
        "            ['Data Magna', 25, 3],\r\n",
        "            ['Sexta-Feira Santa', 10, 4],\r\n",
        "            ['Aniversário de Fortaleza', 13, 4],\r\n",
        "            ['Tiradentes', 21, 4],\r\n",
        "            ['Dia do Trabalho', 1, 5],\r\n",
        "            ['Corpus Christi', 28, 5],\r\n",
        "            ['N. Senhora da Assunção', 27, 5],\r\n",
        "            ['Independência do Brasil', 7, 9],\r\n",
        "            ['N. Senhora de Aparecida', 12, 10],\r\n",
        "            ['Dia de Finados', 2, 11],\r\n",
        "            ['Proclamação da Republica', 15, 11],\r\n",
        "            ['Natal', 25, 12],\r\n",
        "]\r\n",
        "\r\n",
        "vesperas = [\r\n",
        "            ['Ano Novo', 31, 12],\r\n",
        "            ['Carnaval', 23, 2],\r\n",
        "            ['Dia de São José', 18, 3],\r\n",
        "            ['Data Magna', 24, 3],\r\n",
        "            ['Sexta-Feira Santa', 9, 4],\r\n",
        "            ['Aniversário de Fortaleza', 12, 4],\r\n",
        "            ['Tiradentes', 20, 4],\r\n",
        "            ['Dia do Trabalho', 30, 4],\r\n",
        "            ['N. Senhora da Assunção', 26, 5],\r\n",
        "            ['Independência do Brasil', 6, 9],\r\n",
        "            ['N. Senhora de Aparecida', 11, 10],\r\n",
        "            ['Dia de Finados', 1, 11],\r\n",
        "            ['Proclamação da Republica', 14, 11],\r\n",
        "            ['Natal', 24, 12],\r\n",
        "]"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1632360927873
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_model"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360928431
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "one_hot_encoder = OneHotEncoder(sparse=False)\r\n",
        "\r\n",
        "data_model[['domingo','segunda', 'terca', 'quarta', 'quinta', 'sexta', 'sabado']] = one_hot_encoder.fit_transform(data_model['d_semana'].values.reshape(-1,1))\r\n",
        "data_model['feriado'] = [1 if any((x[0] == d and x[1] == m) for (_, d, m) in feriados) else 0 for x in list(zip(data_model.d_mes, data_model.mes))]\r\n",
        "data_model['vespera_feriado'] = [1 if any((x[0] == d and x[1] == m) for (_, d, m) in vesperas) else 0 for x in list(zip(data_model.d_mes, data_model.mes))]\r\n",
        "\r\n",
        "top100_linhas = data_model.linha.value_counts().index[:100]\r\n",
        "top100_linhas_data_model = data_model[data_model.linha.isin(top100_linhas)].loc[data['mes'] != 1]\r\n",
        "\r\n",
        "line_data_model = data_model.loc[data['linha'] == busline_filter].loc[data['mes'] != 1].copy()"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1632360940056
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "top100_linhas_data_model.to_csv(\"./dados-para-modelos/top100_linhas_data_model.csv\", index=False)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#display(X)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360940459
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#display(y)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360940845
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------- Fim Setup ----------------------------------------------------------------------------  \r\n",
        "# Criacao dos Modelos"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_list = []"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360941131
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_performance(model):\r\n",
        "    y_test_predict = model.predict(X_test)\r\n",
        "    mse = mean_squared_error(Y_test, y_test_predict)\r\n",
        "    rmse = (np.sqrt(mse))\r\n",
        "    r2 = r2_score(Y_test, y_test_predict)\r\n",
        "    mae = mean_absolute_error(Y_test, y_test_predict)\r\n",
        "    #mape = mean_absolute_percentage_error(Y_test, y_test_predict)\r\n",
        "    performance_scoring = [\r\n",
        "        r2,\r\n",
        "        rmse,\r\n",
        "        mae,\r\n",
        "        #mape\r\n",
        "    ]\r\n",
        "    #performance_scoring = pd.DataFrame(performance_scoring,columns=['Metrica', 'Score'])\r\n",
        "    #performance_scoring['Score'] = performance_scoring['Score'].astype('float64')\r\n",
        "    return performance_scoring"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360941444
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "LinearRegressionModel = LinearRegression()\r\n",
        "RandomForestModel = RandomForestRegressor()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360941799
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Regression Model Parameters"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# print(\"Linear Regression  Params ----> \")\r\n",
        "# pprint(LinearRegressionModel.get_params())\r\n",
        "\r\n",
        "copy_X = [True, False]\r\n",
        "fit_intercept = [True, False]\r\n",
        "n_jobs = [2, 4, 6, 8]\r\n",
        "normalize = [True, False]\r\n",
        "\r\n",
        "# Create the random grid\r\n",
        "random_grid_linearregression = {'copy_X': copy_X, 'fit_intercept': fit_intercept, 'n_jobs': n_jobs, 'normalize': normalize}"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360942004
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Model Parameters"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# print(\"Random Forest Model Params ----> \")\r\n",
        "# pprint(RandomForestModel.get_params())\r\n",
        "\r\n",
        "# Number of trees in random forest\r\n",
        "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 1000, num = 10)]\r\n",
        "\r\n",
        "n_jobs = [2, 4, 6, 8]\r\n",
        "\r\n",
        "# Number of features to consider at every split\r\n",
        "max_features = ['auto', 'sqrt']\r\n",
        "\r\n",
        "# Maximum number of levels in tree\r\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\r\n",
        "max_depth.append(None)\r\n",
        "\r\n",
        "# Minimum number of samples required to split a node\r\n",
        "min_samples_split = [2, 5, 10]\r\n",
        "\r\n",
        "# Minimum number of samples required at each leaf node\r\n",
        "min_samples_leaf = [1, 2, 4]\r\n",
        "\r\n",
        "# Method of selecting samples for training each tree\r\n",
        "bootstrap = [True, False]\r\n",
        "\r\n",
        "# Create the random grid\r\n",
        "random_grid_randomforest = {'n_estimators': n_estimators,\r\n",
        "               'max_features': max_features,\r\n",
        "               'max_depth': max_depth,\r\n",
        "               'min_samples_split': min_samples_split,\r\n",
        "               'min_samples_leaf': min_samples_leaf,\r\n",
        "               'bootstrap': bootstrap,\r\n",
        "               'n_jobs': n_jobs}"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632360942214
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X = line_data_model.filter(feature_names, axis=1)\r\n",
        "y = line_data_model.validations_per_hour\r\n",
        "\r\n",
        "# Random search of parameters, using 3 fold cross validation, \r\n",
        "# search across 100 different combinations, and use all available cores\r\n",
        "LinearRegressionModel = LinearRegression()\r\n",
        "RandomForestModel = RandomForestRegressor()\r\n",
        "gridsearch_result_randomforest = RandomizedSearchCV(estimator = RandomForestModel, param_distributions = random_grid_randomforest, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\r\n",
        "gridsearch_result_linearregression = RandomizedSearchCV(estimator = LinearRegressionModel, param_distributions = random_grid_linearregression, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\r\n",
        "\r\n",
        "# Fit the random search model\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\r\n",
        "\r\n",
        "gridsearch_result_randomforest.fit(X_train, Y_train)\r\n",
        "gridsearch_result_linearregression.fit(X_train, Y_train)\r\n",
        "\r\n",
        "print(\"Random Forest Best Parameters -----> \")\r\n",
        "pprint(gridsearch_result_randomforest.best_params_)\r\n",
        "\r\n",
        "print(\"Linear Regression Best Parameters -----> \")\r\n",
        "pprint(gridsearch_result_linearregression.best_params_)\r\n",
        "\r\n",
        "gridsearched_random_randomforest = gridsearch_result_randomforest.best_estimator_\r\n",
        "gridsearch_random_linearregression = gridsearch_result_linearregression.best_estimator_\r\n",
        "\r\n",
        "default_linearregression_model = LinearRegressionModel.fit(X_train, Y_train)\r\n",
        "default_randomforest_model = RandomForestModel.fit(X_train, Y_train)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632361324253
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sample = X.sample(n=1)\r\n",
        "sample"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632361324634
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"###  1 Modelo Por Linha (dado completo de treino)\")\r\n",
        "\r\n",
        "performance_default_linearregression_model = get_performance(default_linearregression_model)\r\n",
        "performance_default_randomforest_model = get_performance(default_randomforest_model)\r\n",
        "performance_gridsearched_linearregression_model = get_performance(gridsearch_random_linearregression)\r\n",
        "performance_gridsearched_randomforest_model = get_performance(gridsearched_random_randomforest)\r\n",
        "\r\n",
        "print(\"\\n----------------------------------------------------\\n\")\r\n",
        "print('Regressao Linear [DEFAULT]: \\n', performance_default_linearregression_model)\r\n",
        "print(\"\\n----------------------------------------------------\\n\")\r\n",
        "print('Random Forest [DEFAULT]: \\n', performance_default_randomforest_model)\r\n",
        "print(\"\\n----------------------------------------------------\\n\")\r\n",
        "print('Regressao Linear [GRID SEARCH]: \\n', performance_gridsearched_linearregression_model)\r\n",
        "print(\"\\n----------------------------------------------------\\n\")\r\n",
        "print('Random Forest [GRID SEARCH]: \\n', performance_gridsearched_randomforest_model)\r\n",
        "print(\"\\n----------------------------------------------------\\n\")\r\n",
        "\r\n",
        "print(\"\\nUtilizando sample aleatorio de dado para teste de previsao: \")\r\n",
        "\r\n",
        "predict_res = default_linearregression_model.predict(sample)\r\n",
        "\r\n",
        "print(\"Regressao Linear -> resultado do predict de test: \", predict_res)\r\n",
        "\r\n",
        "predict_res2 = default_randomforest_model.predict(sample)\r\n",
        "print(\"Random Forest -> resultado do predict de test: \", predict_res2)\r\n",
        "\r\n",
        "random_res = gridsearch_random_linearregression.predict(sample)\r\n",
        "print(\"Regressao Linear Best Grid Searched -> resultado do predict de test: \", predict_res2)\r\n",
        "\r\n",
        "random_res = gridsearched_random_randomforest.predict(sample)\r\n",
        "print(\"Random Forest Best Grid Searched -> resultado do predict de test: \", predict_res2)\r\n",
        "\r\n",
        "display(data_model.loc[sample.index[0]:sample.index[0]])\r\n",
        "\r\n",
        "print('LINHA UTILIZADA PARA OS TESTES: ', busline_filter)\r\n",
        "\r\n",
        "performance_total = [performance_default_linearregression_model,\r\n",
        "    performance_default_randomforest_model,\r\n",
        "    performance_gridsearched_linearregression_model,\r\n",
        "    performance_gridsearched_randomforest_model]\r\n",
        "\r\n",
        "performance_total = pd.DataFrame.from_records(performance_total, columns=['R2', 'RMSE', 'MAE'], \r\n",
        "    index=['LinearReg Default', 'RandomForest Default', 'LinearReg GridSearched', 'RandomForest GridSearched'])\r\n",
        "performance_total"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1632361324961
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BAGGING"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\r\n",
        "warnings.filterwarnings(\"ignore\") # To ignore warnings\r\n",
        "n_jobs = -1 # This parameter conrols the parallel processing. -1 means using all processors.\r\n",
        "random_state = 42 # This parameter controls the randomness of the data. Using some int value to get same results everytime this code is run\r\n",
        "models_scores = [] # To store model scores               \r\n",
        "\r\n",
        "def bagging_model(estimator):\r\n",
        "    \"\"\"\r\n",
        "    I/P\r\n",
        "    estimator: The base estimator from which the ensemble is grown.\r\n",
        "    O/P\r\n",
        "    br_y_pred: Predictions on test data for the base estimator.\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    regr = BaggingRegressor(base_estimator=estimator,\r\n",
        "                            n_estimators=10,\r\n",
        "                            max_samples=1.0,\r\n",
        "                            bootstrap=True, # Samples are drawn with replacement\r\n",
        "                            n_jobs= n_jobs,\r\n",
        "                            random_state=random_state).fit(X_train, Y_train)\r\n",
        "\r\n",
        "    br_y_pred = regr.predict(X_test)\r\n",
        "\r\n",
        "    performance = get_performance(regr)\r\n",
        "    \r\n",
        "    print(f'Performance for base estimator {regr.base_estimator_} = {performance}\\n')\r\n",
        "\r\n",
        "    return regr, performance\r\n",
        "\r\n",
        "LinearRegressionModelPipeline = make_pipeline(LinearRegression())\r\n",
        "RandomForestModelPipeline = make_pipeline(RandomForestRegressor())\r\n",
        "\r\n",
        "bagging_lr_model, performance_bagging_lr_model = bagging_model(LinearRegressionModelPipeline)\r\n",
        "bagging_rf_model, performance_bagging_rf_model = bagging_model(RandomForestModelPipeline)\r\n",
        "\r\n",
        "df2 = pd.DataFrame([performance_bagging_lr_model, performance_bagging_rf_model], columns=['R2','RMSE','MAE'], \r\n",
        "    index=['LinearRegression Bagging', 'RandomForest Bagging'])\r\n",
        "performance_total = pd.concat([df2, performance_total])\r\n"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632361332525
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STACKING"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "estimators = [ ('random_forest', RandomForestModelPipeline), ('linear_reg', LinearRegressionModelPipeline)]\r\n",
        "\r\n",
        "stack = StackingRegressor(estimators=estimators, final_estimator= RandomForestModelPipeline, cv= 5, n_jobs= n_jobs, passthrough = True)\r\n",
        "\r\n",
        "stack.fit(X_train, Y_train)\r\n",
        "\r\n",
        "performance = get_performance(stack)\r\n",
        "\r\n",
        "df2 = pd.DataFrame([performance], columns=['R2','RMSE','MAE'], \r\n",
        "    index=['Stacking Regressors'])\r\n",
        "\r\n",
        "performance_total = pd.concat([df2, performance_total])\r\n"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632361341280
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BOOSTING"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gradient_boosting_regressor= GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\r\n",
        "                                   max_depth=4, max_features='sqrt',\r\n",
        "                                   min_samples_leaf=15, min_samples_split=10, \r\n",
        "                                   loss='huber', random_state = random_state)\r\n",
        "\r\n",
        "gradient_boosting_regressor.fit(X_train, Y_train)\r\n",
        "\r\n",
        "performance = get_performance(gradient_boosting_regressor)\r\n",
        "\r\n",
        "df2 = pd.DataFrame([performance], columns=['R2','RMSE','MAE'], \r\n",
        "    index=['GradientBoostingRegressor'])\r\n",
        "\r\n",
        "performance_total = pd.concat([df2, performance_total])\r\n"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632361361458
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "performance_total.to_csv('./performances/performance_total_linha'+str(busline_filter)+'.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632369757434
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "performance_total.sort_values('MAE', ascending=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Todos os modelos treinados dentro de *model_list*"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_list = [\r\n",
        "    stack,\r\n",
        "    gradient_boosting_regressor,\r\n",
        "    bagging_rf_model,\r\n",
        "    default_randomforest_model,\r\n",
        "    #default_linearregression_model,\r\n",
        "    #gridsearch_result_linearregression,\r\n",
        "    #gridsearch_result_randomforest,\r\n",
        "    #bagging_lr_model,\r\n",
        "]"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632363793252
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinar com 1 mes, prever N semanas"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "# Jan, Fev, Mar, Abril, Maio\r\n",
        "# treina com mar, preve 2 semanas de abril\r\n",
        "\r\n",
        "mes_de_treino = 5\r\n",
        "mes_de_previsao = 11\r\n",
        "\r\n",
        "line_mes_data_model = line_data_model[line_data_model.mes == mes_de_treino]\r\n",
        "\r\n",
        "line_mes_data_model_predict = line_data_model[line_data_model.mes == mes_de_previsao]\r\n",
        "\r\n",
        "df_prev = line_mes_data_model_predict[line_mes_data_model_predict.semana_do_mes <=  2].filter(feature_names, axis = 1)\r\n",
        "real_values = pd.DataFrame(line_mes_data_model_predict[line_mes_data_model_predict.semana_do_mes <=  2].validations_per_hour)\r\n",
        "\r\n",
        "X = line_mes_data_model.filter(feature_names, axis=1)\r\n",
        "y = line_mes_data_model.validations_per_hour\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\r\n",
        "\r\n",
        "predictions = [[],[],[],[]]\r\n",
        "performances = list()\r\n",
        "df_prev\r\n",
        "for i, model in enumerate(model_list):\r\n",
        "        model.fit(X_train, Y_train)\r\n",
        "        performances.append(get_performance(model))\r\n",
        "\r\n",
        "        for index, row in df_prev.iterrows():\r\n",
        "            predictions[i].append(model.predict([row])[0])\r\n",
        "\r\n",
        "df_performance = pd.DataFrame(performances)\r\n",
        "df_performance.to_csv('./performances/linha'+str(busline_filter)+'_1mes('+str(mes_de_treino)+')_2semanas('+str(mes_de_previsao)+')_performance.csv', index=False)\r\n",
        "df_predict_dados = real_values\r\n",
        "df_predict_dados['validations_per_hour_predicted'] = predictions[0]\r\n",
        "df_predict_dados.to_csv('./predict-vs-real/linha'+str(busline_filter)+'_1mes('+str(mes_de_treino)+')_2semanas('+str(mes_de_previsao)+')_predict.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632364536684
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinar com (N-Y) meses, Prever mes (N)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#X_train, Y_train\r\n",
        "#X_test, Y_test\r\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\r\n",
        "# X = line_data_model.filter(feature_names, axis=1)\r\n",
        "# y = line_data_model.validations_per_hour\r\n",
        "\r\n",
        "#quantos meses (comecando do final do dado) vou prever\r\n",
        "\r\n",
        "# model_list = [\r\n",
        "#     default_linearregression_model,\r\n",
        "#     default_randomforest_model,\r\n",
        "#     gridsearch_result_linearregression,\r\n",
        "#     gridsearch_result_randomforest,\r\n",
        "#     bagging_lr_model,\r\n",
        "#     bagging_rf_model,\r\n",
        "#     stack,\r\n",
        "#     gradient_boosting_regressor\r\n",
        "# ]\r\n",
        "\r\n",
        "for hzp in range(4, 8):\r\n",
        "    horizonte_de_previsao = hzp\r\n",
        "    meses = line_data_model.mes.sort_values().unique()\r\n",
        "    horizonte_de_treinamento = meses[:meses.size - horizonte_de_previsao]\r\n",
        "    meses_de_previsao = meses[meses.size - horizonte_de_previsao:]\r\n",
        "\r\n",
        "    predictions = [[],[],[],[],[],[],[],[]]\r\n",
        "    performances = [[],[],[],[],[],[],[],[]]\r\n",
        "\r\n",
        "    df_treinamento = line_data_model[line_data_model.mes.isin(horizonte_de_treinamento)].copy()\r\n",
        "    df_previsao = line_data_model[line_data_model.mes.isin(meses_de_previsao)].copy()\r\n",
        "    df_prev = df_previsao.filter(feature_names, axis = 1)\r\n",
        "    df_results = df_previsao.filter('validations_per_hour', axis = 1)\r\n",
        "\r\n",
        "    print(df_results.__len__())\r\n",
        "\r\n",
        "    X = df_treinamento.filter(feature_names, axis=1)\r\n",
        "    y = df_treinamento.validations_per_hour\r\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\r\n",
        "\r\n",
        "    for i, model in enumerate(model_list):\r\n",
        "        model.fit(X_train, Y_train)\r\n",
        "        performances[i].append(get_performance(model))\r\n",
        "\r\n",
        "        for index, row in df_prev.iterrows():\r\n",
        "            predictions[i].append(model.predict([row])[0])\r\n",
        "    \r\n",
        "    df_results = df_previsao.validations_per_hour\r\n",
        "\r\n",
        "    df_performance = pd.DataFrame(performances)\r\n",
        "    df_performance.to_csv('./performances/performance_'+str(hzp)+'_meses.csv', index=False)\r\n",
        "\r\n",
        "    plt_size = hzp * 360\r\n",
        "\r\n",
        "    plt.figure(figsize=(30, 10))\r\n",
        "    plt.plot(range(len(df_results) - plt_size), df_results[plt_size:], color='blue')\r\n",
        "    plt.plot(range(len(predictions[0]) - plt_size ), predictions[2][plt_size:], color='red')\r\n",
        "    plt.xlabel('True Values')\r\n",
        "    plt.ylabel('Predictions')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "#print(predictions[0])\r\n"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631467016584
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(figsize=(30, 10))\r\n",
        "plt.plot(range(len(df_results) - plt_size), df_results[plt_size:], color='blue')\r\n",
        "plt.plot(range(len(predictions[2]) - plt_size ), predictions[0][plt_size:], color='red')\r\n",
        "plt.xlabel('True Values')\r\n",
        "plt.ylabel('Predictions')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631409753552
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "performances"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631409753830
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = pd.DataFrame(predictions)\r\n",
        "df.to_csv('predictions0.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631409754004
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# ACCESS IN ORDER PREDICTION VALUES\r\n",
        "df.iloc[0]"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631409754185
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# #top 100 exampled linhas\r\n",
        "# #data_model.linha.value_counts().index[:100]\r\n",
        "\r\n",
        "# print(data_model.linha.value_counts().index[:100])\r\n",
        "\r\n",
        "d_31 = [1, 3, 5, 7, 8, 10, 12]\r\n",
        "d_30 = [4, 6, 9, 11]\r\n",
        "\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "import datetime\r\n",
        "import calendar\r\n",
        "\r\n",
        "def week_of_month(tgtdate):\r\n",
        "    tgtdate = tgtdate.to_pydatetime()\r\n",
        "    startdate = 0\r\n",
        "\r\n",
        "    days_this_month = calendar.mdays[tgtdate.month]\r\n",
        "    for i in range(1, days_this_month):\r\n",
        "        d = datetime.datetime(tgtdate.year, tgtdate.month, i)\r\n",
        "        if d.day - d.weekday() > 0:\r\n",
        "            startdate = d\r\n",
        "            break\r\n",
        "    # now we canuse the modulo 7 appraoch\r\n",
        "    return (tgtdate - startdate).days //7 + 1\r\n",
        "\r\n",
        "# #ate outubro para prever novembro\r\n",
        "# for linha in top100_linhas:\r\n",
        "#     #linha filter\r\n",
        "#     currentLinhaData = data_model[data_model.linha == linha]\r\n",
        "#     print(currentLinhaData)\r\n",
        "#     for mes in range(3, 12):\r\n",
        "#         currentLinhaData = currentLinhaData[currentLinhaData.mes == mes]\r\n",
        "\r\n",
        "#         if mes in d_31:\r\n",
        "#             d_d = 31\r\n",
        "#         elif mes in d_30:\r\n",
        "#             d_d = 30\r\n",
        "#         else:\r\n",
        "#             d_d = 29\r\n",
        "        \r\n",
        "#         for dia in range(1, d_d):\r\n",
        "#             currentLinhaData = currentLinhaData[currentLinhaData.d_mes == dia]\r\n",
        "#             for hora in range(24):\r\n",
        "#                 currentLinhaData = currentLinhaData[currentLinhaData.hora == hora]\r\n",
        "#                 if currentLinhaData.empty:\r\n",
        "#                     # feature_names = [\r\n",
        "#                     # 'hour_sin', 'hour_cos', \r\n",
        "#                     # 'd_mes', 'mes', 'semana_do_mes', \r\n",
        "#                     # 'domingo','segunda', 'terca', 'quarta', 'quinta', 'sexta', 'sabado', \r\n",
        "#                     # 'feriado', 'vespera_feriado']\r\n",
        "#                     # target = 'validations_per_hour'\r\n",
        "\r\n",
        "#                     # a_row = pd.Series([1, 2])\r\n",
        "#                     # df = pd.DataFrame([[3, 4], [5, 6]])\r\n",
        "#                     # row_df = pd.DataFrame([a_row])\r\n",
        "#                     # df = pd.concat([row_df, df], ignore_index=True)\r\n",
        "\r\n",
        "#                     h_sin = np.sin(2 * np.pi * hora/23.0)\r\n",
        "#                     h_cos = np.cos(2 * np.pi * hora/23.0)\r\n",
        "#                     semana_do_mes = (dia-1) // 7 + 1\r\n",
        "#                     #dia do ano\r\n",
        "#                     ins = pd.Series([linha, data_hora, 0, h_sin, h_cos, dia, mes, semana_do_mes *DIASDASEMANAENCODED, *feriado, *vesperaferiado])\r\n",
        "#                     ins_df = pd.DataFrame([ins])\r\n",
        "\r\n",
        "#                     currentLinhaData = pd.concat([ins_df, currentLinhaData], ignore_index=True)\r\n",
        "#                     print(f\"INSERINDO MISSING HORA {hora} no dia {dia} do mes {mes}\")\r\n",
        "\r\n",
        "# t = data_model[data_model.linha.isin(data_model.linha.value_counts().index[:100])]\r\n",
        "# t"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631409754438
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking Linhas de Onibus"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "# print(data_model.linha.value_counts(ascending=True)[data_model['linha'].value_counts() < 100])\r\n",
        "\r\n",
        "# linha_lista = data_model.linha.unique()\r\n",
        "# linha_lista = linha_lista[:10]\r\n",
        "res_map = list()\r\n",
        "for linha in top100_linhas:\r\n",
        "    for model in model_list:\r\n",
        "        \r\n",
        "        # X = currentLinhaData.filter(['hour_sin', 'hour_cos', \r\n",
        "        # 'd_mes', 'd_ano', 'mes', 'semana_do_mes', \r\n",
        "        # 'domingo','segunda', 'terca', 'quarta', 'quinta', 'sexta', 'sabado', \r\n",
        "        # 'feriado', 'vespera_feriado'], axis=1)\r\n",
        "        # y = currentLinhaData.validations_per_hour\r\n",
        "\r\n",
        "        currentLinhaData = top100_linhas_data_model.loc[data['linha'] == linha].copy()\r\n",
        "        X = currentLinhaData.filter(feature_names, axis=1)\r\n",
        "        y = currentLinhaData.validations_per_hour\r\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\r\n",
        "\r\n",
        "        model.fit(X_train, Y_train)\r\n",
        "        performance = get_performance(model)\r\n",
        "        performance.insert(0, linha)\r\n",
        "        res_map.append(performance)\r\n",
        "res_map = pd.DataFrame(res_map, columns=[\"Linha\", \"R2\", \"RMSE\", \"MAE\"])\r\n",
        "r = res_map.sort_values('MAE', ascending=True)\r\n",
        "r.to_csv(\"./performances/ranking_top100.csv\", index=False)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1631409754617
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "r.to_csv(\"./performances/ranking_top100.csv\", index=False)\r\n",
        "r"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631504448928
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# res_map = pd.DataFrame(res_map,columns=['Linha', \r\n",
        "#     # '[LR][DEF]R2', '[LR][DF]RMSE', '[LR][DF]MAE',\r\n",
        "#     '[RF][GS]R2', '[RF][GS]RMSE', '[RF][GS]MAE',\r\n",
        "#     # '[LR][BAG]R2', '[LR][BAG]RMSE', '[LR][BAG]MAE',\r\n",
        "#     '[RF][BAG]R2', '[RF][BAG]RMSE', '[RF][BAG]MAE',\r\n",
        "#     '[STK]R2', '[STK]RMSE', '[STK]MAE',\r\n",
        "#     # '[BOS]R2', '[BOS]RMSE', '[BOS]MAE' \r\n",
        "#     ])\r\n",
        "res_map = pd.DataFrame(res_map,columns=['Linha','R2', 'RMSE', 'MAE'])\r\n",
        "\r\n",
        "#performance_scoring['Score'] = performance_scoring['Score'].astype('float64')\r\n",
        "res_map = res_map.sort_values('R2').reset_index(drop=True)\r\n",
        "res_map.to_csv('./top100linhas_rank.csv', index=False)\r\n",
        "res_map"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631504114908
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliacao dos dados atualmente\r\n",
        "404 linhas  \r\n",
        "358 com pelo menos 10 exemplos\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# res_map.to_csv('./classic-linha-comp.csv', index = False)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1631409754780
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "4caa081543ae4d7454eea2adf6270962382d99fc25b3a6423caed7aecce4d7f8"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('ml': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}